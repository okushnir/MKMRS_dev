#!/usr/bin/env python
"""
@Created by: [Your Name]
@Based on: Tagging.py by Oded Kushnir for Biodaat

A script for processing shark CPUE (Catch Per Unit Effort) data from transect files.
This script focuses on correctly extracting all visual tags and matching them with deployments.
"""
import argparse
import glob
import os
import pandas as pd
import re
import sys


def read_file_with_encoding(file_path):
    """
    Read a file with multiple encoding attempts.

    Args:
        file_path (str): Path to file

    Returns:
        str: File content as string or None if all encodings fail
    """
    encodings = ['utf-8-sig', 'cp1252', 'latin1', 'iso-8859-1']

    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                content = file.read()
                print(f"Successfully read file with encoding: {encoding}")
                return content, encoding
        except UnicodeDecodeError:
            print(f"Failed to read with encoding {encoding}, trying another...")

    print(f"ERROR: Could not read file {file_path} with any encoding")
    return None, None


def extract_sections(content):
    """
    Extract sections from a file based on [SectionName] headers.

    Args:
        content (str): File content as string

    Returns:
        dict: Dictionary with section names as keys and section content as values
    """
    if not content:
        return {}

    # Split content into lines
    lines = content.split('\n')

    # Find all section headers and their line numbers
    section_markers = []
    for i, line in enumerate(lines):
        match = re.match(r'^\s*\[([^\]]+)\]', line)
        if match:
            section_name = match.group(1)
            section_markers.append((i, section_name))

    # Extract content between section headers
    sections = {}
    for i in range(len(section_markers)):
        start_idx = section_markers[i][0] + 1  # Skip the header line
        end_idx = section_markers[i + 1][0] if i < len(section_markers) - 1 else len(lines)
        section_name = section_markers[i][1]

        # Extract section content
        section_content = '\n'.join(lines[start_idx:end_idx])
        sections[section_name] = section_content

        # Report success
        print(f"Extracted section: {section_name} ({end_idx - start_idx} lines)")

    return sections


def parse_section_as_dataframe(section_content):
    """
    Parse a section's content as a CSV and return as DataFrame.

    Args:
        section_content (str): Section content

    Returns:
        pd.DataFrame: DataFrame with parsed content
    """
    if not section_content.strip():
        return pd.DataFrame()

    # Parse the CSV content
    try:
        # Use pandas to parse the CSV
        df = pd.read_csv(pd.io.common.StringIO(section_content),
                         skip_blank_lines=True,
                         on_bad_lines='warn')  # Fixed parameter here

        # Clean up column names - strip whitespace
        df.columns = [str(col).strip() for col in df.columns]

        # Rename any unnamed columns
        for i, col in enumerate(df.columns):
            if 'Unnamed:' in col:
                df = df.rename(columns={col: f'column_{i}'})

        # Drop rows where all values are NaN or empty string
        df = df.dropna(how='all')
        df = df[~df.astype(str).apply(lambda x: x.str.strip().eq(''), axis=1).all(axis=1)]

        return df
    except Exception as e:
        print(f"Error parsing section as DataFrame: {e}")
        return pd.DataFrame()


def process_file(file_path):
    """
    Process a single file to extract relevant sections.

    Args:
        file_path (str): Path to file

    Returns:
        dict: Dictionary of DataFrames for each section
    """
    print(f"\nProcessing file: {file_path}")
    filename = os.path.basename(file_path)

    # Read file content
    content, encoding = read_file_with_encoding(file_path)
    if not content:
        return {}

    # Extract sections
    sections = extract_sections(content)

    # Parse each section to DataFrame
    dataframes = {}
    for section_name, section_content in sections.items():
        df = parse_section_as_dataframe(section_content)

        if not df.empty:
            # Add source file information
            df['source_file'] = filename

            # Extract date from filename if available (format: transect_YYYYMMDD_...)
            date_match = re.search(r'transect_(\d{8})_', filename)
            if date_match:
                date_str = date_match.group(1)
                df['file_date'] = f"{date_str[6:8]}/{date_str[4:6]}/{date_str[0:4]}"

            # Store in results dictionary
            dataframes[section_name] = df
            print(f"  Section '{section_name}': {df.shape[0]} rows, {df.shape[1]} columns")
        else:
            print(f"  Section '{section_name}' is empty or could not be parsed")

    return dataframes


def extract_site_from_filename(filename):
    """
    Extract site information from filename.

    Args:
        filename (str): Filename to extract site from

    Returns:
        str: Site name
    """
    # Extract site part from filename (format: transect_YYYYMMDD_XXX_SiteName.csv)
    site_match = re.search(r'_([^_]+)\.csv$', filename)
    if site_match:
        site_code = site_match.group(1)

        # Map site codes to actual names
        site_mapping = {
            'DakarRaphi': 'Hadera',
            'Dakar-Raphi': 'Hadera',
            'Salit': 'Ashdod Power Plant',
            'Adva-Boston': 'Ashdod Power Plant',
            'Rami-Zadok': 'Ashkelon',
            'Michmoret1': 'Michmoret'
        }

        return site_mapping.get(site_code, site_code)

    return None


def process_lat_lng(value):
    """
    Process latitude and longitude from combined format.

    Args:
        value: String containing lat/lng

    Returns:
        tuple: (latitude, longitude) as floats or (None, None) if parsing fails
    """
    if pd.isna(value) or not value:
        return None, None

    # Convert to string and split by space
    value_str = str(value).strip()
    parts = value_str.split()

    if len(parts) >= 2:
        try:
            lat = float(parts[0])
            lng = float(parts[1])
            return lat, lng
        except (ValueError, TypeError):
            pass

    return None, None


def parse_datetime_safely(value):
    """
    Parse datetime string with multiple format attempts.

    Args:
        value: Value to parse

    Returns:
        datetime or None: Parsed datetime or None if parsing fails
    """
    if pd.isna(value) or not value or value in ['NA', 'nan', 'None', 'NaN']:
        return None

    # Convert to string if not already
    if not isinstance(value, str):
        value = str(value)

    # Try different formats
    formats = [
        '%d/%m/%Y %H:%M:%S',
        '%d/%m/%Y %H:%M',
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%d-%m-%Y %H:%M:%S',
        '%d-%m-%Y %H:%M'
    ]

    for fmt in formats:
        try:
            return pd.to_datetime(value, format=fmt)
        except:
            continue

    # Try pandas default parsing as last resort
    try:
        return pd.to_datetime(value, errors='coerce')
    except:
        return None


def match_captures_with_spagetti(capture_data):
    """
    Extract data records for captures with Spagetti (visual) tags.

    Args:
        capture_data (pd.DataFrame): Data from Megalodon Capture section

    Returns:
        pd.DataFrame: Filtered data with only records containing visual tags
    """
    if capture_data.empty:
        return pd.DataFrame()

    # Ensure Spagetti column exists
    if 'Spagetti' not in capture_data.columns:
        print("Warning: 'Spagetti' column not found in capture data")
        return pd.DataFrame()

    # Convert to string format for filtering
    capture_data['Spagetti'] = capture_data['Spagetti'].astype(str)

    # Filter for valid Spagetti values
    valid_tags = capture_data[
        ~capture_data['Spagetti'].isin(['NA', 'nan', 'None', 'NaN', '']) &
        ~capture_data['Spagetti'].isna() &
        capture_data['Spagetti'].str.strip() != ''
        ].copy()

    print(f"Found {len(valid_tags)} records with Spagetti (visual) tags")

    # Clean the tags - extract numeric part if present
    valid_tags['visual_tag'] = valid_tags['Spagetti'].apply(
        lambda x: ''.join(c for c in str(x) if c.isdigit())
    )

    # Filter out rows where we couldn't extract a numeric part
    valid_tags = valid_tags[valid_tags['visual_tag'] != '']

    # Convert to numeric
    valid_tags['visual_tag'] = pd.to_numeric(valid_tags['visual_tag'], errors='coerce')
    valid_tags = valid_tags.dropna(subset=['visual_tag'])

    if not valid_tags.empty:
        valid_tags['visual_tag'] = valid_tags['visual_tag'].astype(int)
        print(f"Successfully processed {len(valid_tags)} valid numeric visual tags")

        # Print the tag values
        if len(valid_tags) > 0:
            tag_values = sorted(valid_tags['visual_tag'].unique())
            print(f"Visual tag values: {tag_values}")

    return valid_tags


def match_deployments_with_captures(gear_data, captures_with_tags):
    """
    Match gear deployments with captures that have Spagetti tags.

    Args:
        gear_data (pd.DataFrame): Data from Megalodon Gear section
        captures_with_tags (pd.DataFrame): Filtered capture data with visual tags

    Returns:
        pd.DataFrame: CPUE records with deployment and capture information
    """
    if captures_with_tags.empty:
        return pd.DataFrame()

    # Check for required columns
    if not all(col in captures_with_tags.columns for col in ['Capture Method', 'DateTime']):
        print("Warning: Required columns missing in capture data")
        return captures_with_tags

    # Convert gear data columns to appropriate types if not empty
    if not gear_data.empty:
        gear_data['Activity'] = gear_data['Activity'].astype(str)

        # Split deployments and collections
        deployments = gear_data[gear_data['Activity'].str.strip() == 'Deployment'].copy()
        collections = gear_data[gear_data['Activity'].str.strip() == 'Collection'].copy()

        print(f"Found {len(deployments)} deployments and {len(collections)} collections")
    else:
        deployments = pd.DataFrame()
        collections = pd.DataFrame()
        print("No gear deployment/collection data available")

    # Create CPUE records from captures with tags
    cpue_records = []

    for _, capture in captures_with_tags.iterrows():
        record = {}

        # Add the visual tag and basic capture info
        record['visual_tag'] = capture['visual_tag']
        record['event'] = capture.get('Capture State', None)
        record['dep_type'] = capture.get('Capture Method', None)
        record['site'] = extract_site_from_filename(capture.get('source_file', ''))

        # Set capture date as both deployment and collection time initially
        capture_datetime = parse_datetime_safely(capture.get('DateTime'))
        record['dep_date_time'] = capture.get('DateTime', None)
        record['col_date_time'] = capture.get('DateTime', None)

        # Add gear identifier info
        record['dep_drumline_number'] = capture.get('Drumline Number', None)
        record['dep_longline_letter'] = capture.get('Longline Letter', None)
        record['dep_longline_number'] = capture.get('Longline Number', None)

        # Process latitude and longitude
        lat, lng = process_lat_lng(capture.get('Lat Lng'))
        record['dep_lat'] = lat
        record['dep_lon'] = lng

        # Create a gear identifier for matching
        gear_id = None

        if capture.get('Capture Method') == 'Drumline' and not pd.isna(capture.get('Drumline Number')):
            gear_id = ('Drumline', str(capture['Drumline Number']))
        elif (capture.get('Capture Method') == 'Longline' and
              not pd.isna(capture.get('Longline Letter')) and
              not pd.isna(capture.get('Longline Number'))):
            gear_id = ('Longline', f"{capture['Longline Letter']}{capture['Longline Number']}")

        # Try to find a matching deployment
        if gear_id and not deployments.empty and capture_datetime:
            # Filter deployments by type
            type_matches = deployments[deployments['Type'] == gear_id[0]]

            if not type_matches.empty:
                # Match by specific identifier
                if gear_id[0] == 'Drumline':
                    type_matches['Drumline Number'] = type_matches['Drumline Number'].astype(str)
                    matches = type_matches[type_matches['Drumline Number'] == gear_id[1]]
                else:  # Longline
                    valid_letter = type_matches['Longline Letter'].astype(str) == gear_id[1][0]
                    valid_number = type_matches['Longline Number'].astype(str) == gear_id[1][1:]
                    matches = type_matches[valid_letter & valid_number]

                if not matches.empty:
                    # Convert to datetime for comparison
                    matches['deploy_time'] = matches['DateTime'].apply(parse_datetime_safely)

                    # Find the deployment that happened before capture
                    valid_matches = matches[matches['deploy_time'] <= capture_datetime]

                    if not valid_matches.empty:
                        # Get the most recent deployment before capture
                        deployment = valid_matches.sort_values('deploy_time', ascending=False).iloc[0]

                        # Update record with deployment info
                        record['dep_date_time'] = deployment.get('DateTime')
                        record['dep_bait'] = deployment.get('Baits')
                        record['dep_bait_on_off'] = deployment.get('Bait On/Off')
                        record['dep_longline_bottom_surface'] = deployment.get('Longline Bottom/Surface')
                        record['dep_bottom_salinity'] = deployment.get('Bottom Salinity')
                        record['dep_bottom_temperature'] = deployment.get('Bottom Temperature')
                        record['dep_sea_surface_salinity'] = deployment.get('Sea-Surface Salinity')
                        record['dep_sea_surface_temperature'] = deployment.get('Sea-Surface Temperature')
                        record['dep_handline_anchoring_motoring'] = deployment.get('Handline Anchoring/Motoring')
                        record['comments'] = deployment.get('comments')

                        # Try to find a matching collection also
                        if not collections.empty:
                            coll_matches = collections[collections['Type'] == gear_id[0]]

                            if gear_id[0] == 'Drumline':
                                coll_matches['Drumline Number'] = coll_matches['Drumline Number'].astype(str)
                                col_matches = coll_matches[coll_matches['Drumline Number'] == gear_id[1]]
                            else:  # Longline
                                valid_letter = coll_matches['Longline Letter'].astype(str) == gear_id[1][0]
                                valid_number = coll_matches['Longline Number'].astype(str) == gear_id[1][1:]
                                col_matches = coll_matches[valid_letter & valid_number]

                            if not col_matches.empty:
                                col_matches['collect_time'] = col_matches['DateTime'].apply(parse_datetime_safely)

                                # Find collections after capture
                                valid_cols = col_matches[col_matches['collect_time'] >= capture_datetime]

                                if not valid_cols.empty:
                                    # Get the earliest collection after capture
                                    collection = valid_cols.sort_values('collect_time').iloc[0]
                                    record['col_date_time'] = collection.get('DateTime')

        # Add record to list
        cpue_records.append(record)

    # Create DataFrame from records
    if cpue_records:
        result_df = pd.DataFrame(cpue_records)
        # Remove duplicate visual tags (keep first occurrence)
        if 'visual_tag' in result_df.columns:
            result_df = result_df.drop_duplicates(subset=['visual_tag'])
            print(f"Final CPUE records after removing duplicates: {len(result_df)}")
        return result_df

    return pd.DataFrame()


def process_directory(directory):
    """
    Process all transect files in a directory.

    Args:
        directory (str): Path to directory containing transect files

    Returns:
        tuple: (gear_data, capture_data) - Combined DataFrames for each section
    """
    # Lists to store data from each file
    gear_data_list = []
    capture_data_list = []

    # Find all transect files
    file_paths = glob.glob(os.path.join(directory, 'transect_*.csv'))
    print(f"Found {len(file_paths)} transect files in directory")

    for file_path in file_paths:
        # Skip directories
        if os.path.isdir(file_path):
            continue

        # Process the file
        dataframes = process_file(file_path)

        # Extract sections of interest
        if 'Megalodon Gear' in dataframes:
            gear_data_list.append(dataframes['Megalodon Gear'])

        if 'Megalodon Capture' in dataframes:
            capture_data_list.append(dataframes['Megalodon Capture'])

    # Combine the data from all files
    combined_gear_data = pd.DataFrame()
    combined_capture_data = pd.DataFrame()

    if gear_data_list:
        combined_gear_data = pd.concat(gear_data_list, ignore_index=True)
        print(f"Combined gear data: {combined_gear_data.shape[0]} rows")

    if capture_data_list:
        combined_capture_data = pd.concat(capture_data_list, ignore_index=True)
        print(f"Combined capture data: {combined_capture_data.shape[0]} rows")

    return combined_gear_data, combined_capture_data


def format_dates(df, date_columns):
    """
    Format date columns to consistent format.

    Args:
        df (pd.DataFrame): DataFrame with date columns
        date_columns (list): List of column names to format

    Returns:
        pd.DataFrame: DataFrame with formatted date columns
    """
    for col in date_columns:
        if col in df.columns:
            # Create a temporary column with parsed datetime objects
            df[f'{col}_temp'] = df[col].apply(parse_datetime_safely)

            # Format the datetime objects to desired format
            mask = df[f'{col}_temp'].notna()
            if mask.any():
                df.loc[mask, col] = df.loc[mask, f'{col}_temp'].dt.strftime('%d/%m/%Y %H:%M')

            # Remove the temporary column
            df = df.drop(columns=[f'{col}_temp'])

    return df


def main(args):
    """
    Main function to process shark CPUE data.

    Args:
        args: Command line arguments
    """
    delphis_dir = args.delphis_dir
    cpue_dict_file = args.cpue_dict
    cpue_list_file = args.cpue_list
    output_file = args.output_file
    readonly = args.readonly

    print(f"Starting processing with parameters:")
    print(f"  Delphis directory: {delphis_dir}")
    print(f"  CPUE dictionary: {cpue_dict_file}")
    print(f"  CPUE list: {cpue_list_file}")
    print(f"  Output file: {output_file}")
    print(f"  Readonly mode: {readonly}")

    # Load mapping files
    try:
        cpue_dict_df = pd.read_csv(cpue_dict_file)
        cpue_list_df = pd.read_csv(cpue_list_file)

        # Create mapping dictionary
        column_mapping = dict(zip(cpue_dict_df.iloc[:, 0], cpue_dict_df.iloc[:, 1]))
        print(f"Loaded column mapping with {len(column_mapping)} entries")
    except Exception as e:
        print(f"Error loading mapping files: {e}")
        return

    # Process all files in the directory
    gear_data, capture_data = process_directory(delphis_dir)

    if capture_data.empty:
        print("No capture data found. Exiting.")
        return

    # Extract captures with Spagetti tags
    captures_with_tags = match_captures_with_spagetti(capture_data)

    if captures_with_tags.empty:
        print("No captures with visual tags found. Exiting.")
        return

    # Match deployments with captures to create CPUE data
    cpue_data = match_deployments_with_captures(gear_data, captures_with_tags)

    if cpue_data.empty:
        print("No CPUE data generated. Exiting.")
        return

    # Format date columns
    cpue_data = format_dates(cpue_data, ['dep_date_time', 'col_date_time'])

    # Get expected columns from CPUE list
    expected_columns = list(cpue_list_df.iloc[:, 0])

    # Create final output with expected columns
    # Keep only columns that exist in our data
    available_columns = [col for col in expected_columns if col in cpue_data.columns]

    if len(available_columns) < len(expected_columns):
        missing = set(expected_columns) - set(available_columns)
        print(f"Warning: Missing {len(missing)} columns in output: {', '.join(missing)}")

    # Select only the available columns for the final output
    final_table = cpue_data[available_columns].copy()

    # Convert numeric columns to appropriate types
    numeric_columns = [
        'dep_lat', 'dep_lon', 'dep_drumline_number', 'dep_longline_number',
        'dep_bottom_salinity', 'dep_bottom_temperature',
        'dep_sea_surface_salinity', 'dep_sea_surface_temperature'
    ]

    for col in numeric_columns:
        if col in final_table.columns:
            final_table[col] = pd.to_numeric(final_table[col], errors='coerce')

    # Save to file
    if not final_table.empty:
        try:
            # Sort by visual_tag for easier viewing
            if 'visual_tag' in final_table.columns:
                final_table = final_table.sort_values('visual_tag')

            final_table.to_csv(output_file, index=False, float_format='%.6f')
            print(f"Successfully saved {len(final_table)} rows to {output_file}")

            # Print the range of visual tags
            if 'visual_tag' in final_table.columns:
                tags = final_table['visual_tag'].dropna().astype(int).tolist()
                tags.sort()
                print(f"Visual tags included: {tags}")
                print(f"Tag range: {min(tags)} to {max(tags)}")

                # Check for gaps in the sequence
                expected_tags = set(range(min(tags), max(tags) + 1))
                missing_tags = expected_tags - set(tags)
                if missing_tags:
                    print(f"Missing tags in sequence: {sorted(missing_tags)}")
        except Exception as e:
            print(f"Error saving output file: {e}")
    else:
        print("No data to save")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process shark CPUE data")
    parser.add_argument("delphis_dir", type=str, help="Path to directory with Delphis files")
    parser.add_argument("cpue_dict", type=str, help="Path to column mapping dictionary file")
    parser.add_argument("cpue_list", type=str, help="Path to output column list file")
    parser.add_argument("output_file", type=str, help="Path to output file")
    parser.add_argument("--readonly", action="store_true", help="Do not modify input files")

    args = parser.parse_args(sys.argv[1:])
    main(args)